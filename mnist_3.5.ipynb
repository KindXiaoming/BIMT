{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9367f432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "class BioLinear2D(nn.Module):\n",
    "\n",
    "    def __init__(self, in_dim, out_dim, in_fold=1, out_fold=1, out_ring=False):\n",
    "        super(BioLinear2D, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.in_fold = in_fold\n",
    "        self.out_fold = out_fold\n",
    "        assert in_dim % in_fold == 0\n",
    "        assert out_dim % out_fold == 0\n",
    "        \n",
    "        #compute in_cor, shape: (in_dim_sqrt, in_dim_sqrt)\n",
    "        in_dim_fold = int(in_dim/in_fold)\n",
    "        out_dim_fold = int(out_dim/out_fold)\n",
    "        in_dim_sqrt = int(np.sqrt(in_dim_fold))\n",
    "        out_dim_sqrt = int(np.sqrt(out_dim_fold))\n",
    "        x = np.linspace(1/(2*in_dim_sqrt), 1-1/(2*in_dim_sqrt), num=in_dim_sqrt)\n",
    "        X, Y = np.meshgrid(x, x)\n",
    "        self.in_coordinates = torch.tensor(np.transpose(np.array([X.reshape(-1,), Y.reshape(-1,)])), dtype=torch.float)\n",
    "        \n",
    "        # compute out_cor, shape: (out_dim_sqrt, out_dim_sqrt)\n",
    "        if out_ring:\n",
    "            thetas = np.linspace(1/(2*out_dim_fold)*2*np.pi, (1-1/(2*out_dim_fold))*2*np.pi, num=out_dim_fold)\n",
    "            self.out_coordinates = 0.5+torch.tensor(np.transpose(np.array([np.cos(thetas), np.sin(thetas)]))/4, dtype=torch.float)\n",
    "        else:\n",
    "            x = np.linspace(1/(2*out_dim_sqrt), 1-1/(2*out_dim_sqrt), num=out_dim_sqrt)\n",
    "            X, Y = np.meshgrid(x, x)\n",
    "            self.out_coordinates = torch.tensor(np.transpose(np.array([X.reshape(-1,), Y.reshape(-1,)])), dtype=torch.float)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b54bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BioMLP2D(nn.Module):\n",
    "    def __init__(self, in_dim=2, out_dim=2, w=2, depth=2, shp=None, token_embedding=False, embedding_size=None):\n",
    "        super(BioMLP2D, self).__init__()\n",
    "        if shp == None:\n",
    "            shp = [in_dim] + [w]*(depth-1) + [out_dim]\n",
    "            self.in_dim = in_dim\n",
    "            self.out_dim = out_dim\n",
    "            self.depth = depth\n",
    "                 \n",
    "        else:\n",
    "            self.in_dim = shp[0]\n",
    "            self.out_dim = shp[-1]\n",
    "            self.depth = len(shp) - 1\n",
    "        linear_list = []\n",
    "        for i in range(self.depth):\n",
    "            if i == 0:\n",
    "                # for modular addition\n",
    "                #linear_list.append(BioLinear(shp[i], shp[i+1], in_fold=2))\n",
    "                # for regression\n",
    "                linear_list.append(BioLinear2D(shp[i], shp[i+1], in_fold=1))\n",
    "            elif i == self.depth - 1:\n",
    "                linear_list.append(BioLinear2D(shp[i], shp[i+1], in_fold=1, out_ring=True))\n",
    "            else:\n",
    "                linear_list.append(BioLinear2D(shp[i], shp[i+1]))\n",
    "        self.linears = nn.ModuleList(linear_list)\n",
    "        \n",
    "        \n",
    "        if token_embedding == True:\n",
    "            # embedding size: number of tokens * embedding dimension\n",
    "            self.embedding = torch.nn.Parameter(torch.normal(0,1,size=embedding_size))\n",
    "        \n",
    "        self.shp = shp\n",
    "        # parameters for the bio-inspired trick\n",
    "        self.l0 = 0.5 # distance between two nearby layers\n",
    "        self.in_perm = nn.Parameter(torch.tensor(np.arange(int(self.in_dim/self.linears[0].in_fold)), dtype=torch.float))\n",
    "        self.out_perm = nn.Parameter(torch.tensor(np.arange(int(self.out_dim/self.linears[-1].out_fold)), dtype=torch.float))\n",
    "        self.top_k = 30\n",
    "        self.token_embedding = token_embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        shp = x.shape\n",
    "        x = x.reshape(shp[0],-1)\n",
    "        shp = x.shape\n",
    "        in_fold = self.linears[0].in_fold\n",
    "        x = x.reshape(shp[0], in_fold, int(shp[1]/in_fold))\n",
    "        x = x[:,:,self.in_perm.long()]\n",
    "        x = x.reshape(shp[0], shp[1])\n",
    "        f = torch.nn.SiLU()\n",
    "        for i in range(self.depth-1):\n",
    "            x = f(self.linears[i](x))\n",
    "        x = self.linears[-1](x)\n",
    "        \n",
    "        out_perm_inv = torch.zeros(self.out_dim, dtype=torch.long)\n",
    "        out_perm_inv[self.out_perm.long()] = torch.arange(self.out_dim)\n",
    "        x = x[:,out_perm_inv]\n",
    "        #x = x[:,self.out_perm]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_linear_layers(self):\n",
    "        return self.linears\n",
    "    \n",
    "    def get_cc(self, weight_factor=2.0, bias_penalize=True, no_penalize_last=False):\n",
    "        # compute connection cost\n",
    "        cc = 0\n",
    "        num_linear = len(self.linears)\n",
    "        for i in range(num_linear):\n",
    "            if i == num_linear - 1 and no_penalize_last:\n",
    "                weight_factor = 0.\n",
    "            biolinear = self.linears[i]\n",
    "            dist = torch.sum(torch.abs(biolinear.out_coordinates.unsqueeze(dim=1) - biolinear.in_coordinates.unsqueeze(dim=0)),dim=2)\n",
    "            cc += torch.mean(torch.abs(biolinear.linear.weight)*(weight_factor*dist+self.l0))\n",
    "            if bias_penalize == True:\n",
    "                cc += torch.mean(torch.abs(biolinear.linear.bias)*(self.l0))\n",
    "        if self.token_embedding:\n",
    "            cc += torch.mean(torch.abs(self.embedding)*(self.l0))\n",
    "            #pass\n",
    "        return cc\n",
    "    \n",
    "    def swap_weight(self, weights, j, k, swap_type=\"out\"):\n",
    "        with torch.no_grad():  \n",
    "            if swap_type == \"in\":\n",
    "                temp = weights[:,j].clone()\n",
    "                weights[:,j] = weights[:,k].clone()\n",
    "                weights[:,k] = temp\n",
    "            elif swap_type == \"out\":\n",
    "                temp = weights[j].clone()\n",
    "                weights[j] = weights[k].clone()\n",
    "                weights[k] = temp\n",
    "            else:\n",
    "                raise Exception(\"Swap type {} is not recognized!\".format(swap_type))\n",
    "            \n",
    "    def swap_bias(self, biases, j, k):\n",
    "        with torch.no_grad():  \n",
    "            temp = biases[j].clone()\n",
    "            biases[j] = biases[k].clone()\n",
    "            biases[k] = temp\n",
    "    \n",
    "    def swap(self, i, j, k):\n",
    "        # in the ith layer (of neurons), swap the jth and the kth neuron. \n",
    "        # Note: n layers of weights means n+1 layers of neurons.\n",
    "        # (incoming, outgoing) * weights + biases are swapped. \n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i == 0:\n",
    "            return\n",
    "            # for images, do not allow input_perm\n",
    "            # input layer, only has outgoing weights; update in_perm\n",
    "            weights = linears[i].linear.weight\n",
    "            infold = linears[i].in_fold\n",
    "            fold_dim = int(weights.shape[1]/infold)\n",
    "            for l in range(infold):\n",
    "                self.swap_weight(weights, j+fold_dim*l, k+fold_dim*l, swap_type=\"in\")\n",
    "            # change input_perm. do not allow input_perm for images\n",
    "            self.swap_bias(self.in_perm, j, k)\n",
    "        elif i == num_linear:\n",
    "            # output layer, only has incoming weights and biases; update out_perm\n",
    "            weights = linears[i-1].linear.weight\n",
    "            biases = linears[i-1].linear.bias\n",
    "            self.swap_weight(weights, j, k, swap_type=\"out\")\n",
    "            self.swap_bias(biases, j, k)\n",
    "            # change output_perm\n",
    "            self.swap_bias(self.out_perm, j, k)\n",
    "        else:\n",
    "            # middle layer : (incoming, outgoing) * weights, and biases\n",
    "            weights_in = linears[i-1].linear.weight\n",
    "            weights_out = linears[i].linear.weight\n",
    "            biases = linears[i-1].linear.bias\n",
    "            self.swap_weight(weights_in, j, k, swap_type=\"out\")\n",
    "            self.swap_weight(weights_out, j, k, swap_type=\"in\")\n",
    "            self.swap_bias(biases, j, k)\n",
    "\n",
    "    def get_top_id(self, i, top_k=20):\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i == 0:\n",
    "            # input layer\n",
    "            weights = linears[i].linear.weight\n",
    "            score = torch.sum(torch.abs(weights), dim=0)\n",
    "            in_fold = linears[0].in_fold\n",
    "            #print(score.shape)\n",
    "            score = torch.sum(score.reshape(in_fold, int(score.shape[0]/in_fold)), dim=0)\n",
    "        elif i == num_linear:\n",
    "            # output layer\n",
    "            weights = linears[i-1].linear.weight\n",
    "            score = torch.sum(torch.abs(weights), dim=1)\n",
    "        else:\n",
    "            weights_in = linears[i-1].linear.weight\n",
    "            weights_out = linears[i].linear.weight\n",
    "            score = torch.sum(torch.abs(weights_out), dim=0) + torch.sum(torch.abs(weights_in), dim=1)\n",
    "        #print(score.shape)\n",
    "        top_index = torch.flip(torch.argsort(score),[0])[:top_k]\n",
    "        return top_index, score\n",
    "    \n",
    "    def relocate_ij(self, i, j):\n",
    "        # In the ith layer (of neurons), relocate the jth neuron\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i < num_linear:\n",
    "            num_neuron = int(linears[i].linear.weight.shape[1]/linears[i].in_fold)\n",
    "        else:\n",
    "            num_neuron = linears[i-1].linear.weight.shape[0]\n",
    "        ccs = []\n",
    "        for k in range(num_neuron):\n",
    "            self.swap(i,j,k)\n",
    "            ccs.append(self.get_cc())\n",
    "            self.swap(i,j,k)\n",
    "        k = torch.argmin(torch.stack(ccs))\n",
    "        self.swap(i,j,k)\n",
    "            \n",
    "    def relocate_i(self, i):\n",
    "        # Relocate neurons in the ith layer\n",
    "        top_id = self.get_top_id(i, top_k=self.top_k)\n",
    "        for j in top_id[0]:\n",
    "            self.relocate_ij(i,j)\n",
    "            \n",
    "    def relocate(self):\n",
    "        # Relocate neurons in the whole model\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        for i in range(num_linear+1):\n",
    "            self.relocate_i(i)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bf8569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c4eb21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "steps = 40000\n",
    "\n",
    "def cycle(iterable):\n",
    "    while True:\n",
    "        for x in iterable:\n",
    "            yield x\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "train = torchvision.datasets.MNIST(root=\"/tmp\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test = torchvision.datasets.MNIST(root=\"/tmp\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=50, shuffle=True)\n",
    "\n",
    "def accuracy(network, dataset, device, N=2000, batch_size=50):\n",
    "    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for x, labels in islice(dataset_loader, N // batch_size):\n",
    "        #print(x.shape)\n",
    "        logits = network(x.to(device))\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        correct += torch.sum(predicted_labels == labels.to(device))\n",
    "        total += x.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def loss_f(network, dataset, device, N=2000, batch_size=50):\n",
    "    dataset_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    loss = 0\n",
    "    total = 0\n",
    "    for x, labels in islice(dataset_loader, N // batch_size):\n",
    "        logits = network(x.to(device))\n",
    "        loss += torch.sum((logits-torch.eye(10,)[labels])**2)\n",
    "        total += x.size(0)\n",
    "    return loss / total\n",
    "\n",
    "train = torchvision.datasets.MNIST(root=\"/tmp\", train=True, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "test = torchvision.datasets.MNIST(root=\"/tmp\", train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
    "\n",
    "data_size = 60000\n",
    "train = torch.utils.data.Subset(train, range(data_size))\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=100, shuffle=True)\n",
    "\n",
    "def L2(model):\n",
    "    L2_ = 0.\n",
    "    for p in mlp.parameters():\n",
    "        L2_ += torch.sum(p**2)\n",
    "    return L2_\n",
    "\n",
    "def rescale(model, alpha):\n",
    "    for p in mlp.parameters():\n",
    "        p.data = alpha * p.data\n",
    "\n",
    "\n",
    "width = 200\n",
    "mlp = BioMLP2D(shp=(784,100,100,10))\n",
    "\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(mlp.parameters(), lr=1e-3, weight_decay=0.0)\n",
    "\n",
    "one_hots = torch.eye(10, 10).to(device)\n",
    "\n",
    "mlp.eval()\n",
    "print(\"Initial accuracy: {0:.4f}\".format(accuracy(mlp, test, device)))\n",
    "\n",
    "test_accuracies = []\n",
    "train_accuracies = []\n",
    "\n",
    "step = 0\n",
    "mlp.train()\n",
    "pbar = tqdm(islice(cycle(train_loader), steps), total=steps)\n",
    "\n",
    "best_train_loss = 1e4\n",
    "best_test_loss = 1e4\n",
    "best_train_acc = 0.\n",
    "best_test_acc = 0.\n",
    "\n",
    "log = 200\n",
    "lamb = 0.01\n",
    "swap_log = 500\n",
    "plot_log = 500\n",
    "\n",
    "\n",
    "\n",
    "for x, label in pbar:\n",
    "    \n",
    "    if step == int(steps/4):\n",
    "        lamb *= 10\n",
    "    elif step == int(steps/2):\n",
    "        lamb *= 10\n",
    "    \n",
    "    mlp.train()\n",
    "    optimizer.zero_grad()\n",
    "    loss_train = loss_fn(mlp(x.to(device)), one_hots[label])\n",
    "    cc = mlp.get_cc(weight_factor=2.0, no_penalize_last=True)\n",
    "    total_loss = loss_train + lamb*cc\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % log == 0:\n",
    "        with torch.no_grad():\n",
    "            mlp.eval()\n",
    "            train_acc = accuracy(mlp, train, device).item()\n",
    "            test_acc = accuracy(mlp, test, device).item()\n",
    "            train_loss = loss_f(mlp, train, device).item()\n",
    "            test_loss = loss_f(mlp, test, device).item()\n",
    "            \n",
    "            if train_acc > best_train_acc:\n",
    "                best_train_acc = train_acc\n",
    "            if test_acc > best_test_acc:\n",
    "                best_test_acc = test_acc\n",
    "            if train_loss < best_train_loss:\n",
    "                best_train_loss = train_loss\n",
    "            if test_loss < best_test_loss:\n",
    "                best_test_loss = test_loss\n",
    "            mlp.train()\n",
    "            pbar.set_description(\"{:3.3f} | {:3.3f} | {:3.3f} | {:3.3f} | {:3.3f} \".format(train_acc, test_acc, train_loss, test_loss, cc))\n",
    "    step += 1\n",
    "\n",
    "    if step % swap_log == 0:\n",
    "        mlp.relocate()\n",
    "        \n",
    "    if (step-1) % plot_log == 0:\n",
    "        \n",
    "        fig=plt.figure(figsize=(30,15))\n",
    "        ax=fig.add_subplot(projection='3d')\n",
    "\n",
    "        ax.get_proj = lambda: np.dot(Axes3D.get_proj(ax), np.diag([0.5, 0.5, 2, 1]))\n",
    "        ax.scatter(mlp.linears[0].in_coordinates[:,0].detach().numpy(), mlp.linears[0].in_coordinates[:,1].detach().numpy(),[0]*784, s=5, alpha=0.5, c=train[46][0][0].detach().numpy()[:,::-1].reshape(-1,))\n",
    "        ax.scatter(mlp.linears[1].in_coordinates[:,0].detach().numpy(), mlp.linears[1].in_coordinates[:,1].detach().numpy(),[1]*100, s=5, alpha=0.5, color=\"black\")\n",
    "        ax.scatter(mlp.linears[2].in_coordinates[:,0].detach().numpy(), mlp.linears[2].in_coordinates[:,1].detach().numpy(),[2]*100, s=5, alpha=0.5, color=\"black\")\n",
    "        ax.scatter(mlp.linears[2].out_coordinates[:,0].detach().numpy(), mlp.linears[2].out_coordinates[:,1].detach().numpy(),[3]*10, s=5, alpha=0.5, color=\"black\")\n",
    "        ax.set_zlim(-0.5,5)\n",
    "        ax.set_xlim(-0.2,1.2)\n",
    "        ax.set_ylim(-0.2,1.2)\n",
    "\n",
    "\n",
    "        for ii in range(3):\n",
    "            biolinear = mlp.linears[ii]\n",
    "            p = biolinear.linear.weight.clone()\n",
    "            p_shp = p.shape\n",
    "            p = p/torch.abs(p).max()\n",
    "\n",
    "            for i in range(p_shp[0]):\n",
    "                if i % 20 == 0:\n",
    "                    print(i)\n",
    "                for j in range(p_shp[1]):\n",
    "                    out_xy = biolinear.out_coordinates[i].detach().numpy()\n",
    "                    in_xy = biolinear.in_coordinates[j].detach().numpy()\n",
    "                    plt.plot([out_xy[0], in_xy[0]], [out_xy[1], in_xy[1]], [ii+1,ii], lw=1*np.abs(p[i,j].detach().numpy()), color=\"blue\" if p[i,j]>0 else \"red\")\n",
    "\n",
    "\n",
    "        ring = mlp.linears[2].out_coordinates.detach().numpy()\n",
    "        for i in range(10):\n",
    "            ax.text(ring[i,0], ring[i,1], 3.05, \"{}\".format(mlp.out_perm.long()[i].detach().numpy()))\n",
    "\n",
    "\n",
    "        ax.view_init(30,10)\n",
    "\n",
    "        ax.text(0.3,0.25,3.5,\"step={}\".format(step-1), fontsize=15)\n",
    "\n",
    "        ax.axis('off')\n",
    "        \n",
    "        plt.savefig('./results/mnist/{0:06d}.png'.format(step-1))\n",
    "        \n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f984c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,3))\n",
    "\n",
    "for i in range(1,3):\n",
    "    top_k = 784\n",
    "    linears = mlp.get_linear_layers()\n",
    "    num_linear = len(linears)\n",
    "    if i == 0:\n",
    "        # input layer\n",
    "        weights = linears[i].linear.weight\n",
    "        score = torch.sum(torch.abs(weights), dim=0)\n",
    "        in_fold = linears[0].in_fold\n",
    "        #print(score.shape)\n",
    "        score = torch.sum(score.reshape(in_fold, int(score.shape[0]/in_fold)), dim=0)\n",
    "    elif i == num_linear:\n",
    "        # output layer\n",
    "        weights = linears[i-1].linear.weight\n",
    "        score = torch.sum(torch.abs(weights), dim=1)\n",
    "    else:\n",
    "        weights_in = linears[i-1].linear.weight\n",
    "        weights_out = linears[i].linear.weight\n",
    "        score = torch.sum(torch.abs(weights_out), dim=0) + torch.sum(torch.abs(weights_in), dim=1)\n",
    "    #print(score.shape)\n",
    "    top_index = torch.flip(torch.argsort(score),[0])[:top_k]\n",
    "    score = score[top_index]\n",
    "    \n",
    "    num = score.shape[0]\n",
    "    \n",
    "    plt.plot(np.arange(num)+1, score.detach().numpy(), marker=\"o\", markersize=3)\n",
    "    \n",
    "#plt.xscale('log')\n",
    "\n",
    "plt.legend([\"hidden layer 1\", \"hidden layer 2\"], fontsize=15)\n",
    "plt.xlabel(\"Rank\", fontsize=15)\n",
    "plt.ylabel(\"Score\", fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a03382",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "top_k = 784\n",
    "linears = mlp.get_linear_layers()\n",
    "num_linear = len(linears)\n",
    "if i == 0:\n",
    "    # input layer\n",
    "    weights = linears[i].linear.weight\n",
    "    score = torch.sum(torch.abs(weights), dim=0)\n",
    "    in_fold = linears[0].in_fold\n",
    "    #print(score.shape)\n",
    "    score = torch.sum(score.reshape(in_fold, int(score.shape[0]/in_fold)), dim=0)\n",
    "elif i == num_linear:\n",
    "    # output layer\n",
    "    weights = linears[i-1].linear.weight\n",
    "    score = torch.sum(torch.abs(weights), dim=1)\n",
    "else:\n",
    "    weights_in = linears[i-1].linear.weight\n",
    "    weights_out = linears[i].linear.weight\n",
    "    score = torch.sum(torch.abs(weights_out), dim=0) + torch.sum(torch.abs(weights_in), dim=1)\n",
    "#print(score.shape)\n",
    "top_index = torch.flip(torch.argsort(score),[0])[:top_k]\n",
    "score = score[top_index]\n",
    "\n",
    "num = score.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa83746",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = mlp.linears[0].linear.weight[top_index].reshape(100,28,28).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b09e98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "\n",
    "    plt.imshow(features[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"score=%.2f\"%(score[i]), color=\"red\", fontsize=15,y=0.8)\n",
    "    \n",
    "    \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "plt.savefig(\"./fig/mnist_features.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e0a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_f2 = torch.argsort(mlp.linears[2].linear.weight[4])[-1]\n",
    "im_f1 = torch.argsort(mlp.linears[1].linear.weight[im_f2,:])\n",
    "features = mlp.linears[0].linear.weight[im_f1].reshape(100,28,28).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6cde31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "for i in range(100):\n",
    "    plt.subplot(10,10,i+1)\n",
    "\n",
    "    plt.imshow(features[i])\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.title(\"weight=%.2f\"%(mlp.linears[1].linear.weight[im_f2,im_f1[i]]), color=\"red\", fontsize=15,y=0.8)\n",
    "    \n",
    "    \n",
    "plt.subplots_adjust(wspace=0, hspace=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ddaef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,2))\n",
    "\n",
    "nums = [2000,100,20]\n",
    "\n",
    "for i in range(3):\n",
    "\n",
    "    plt.subplot(1,3,i+1)\n",
    "\n",
    "    weights = mlp.linears[i].linear.weight.reshape(-1,)\n",
    "    weights = weights[torch.argsort(weights)]\n",
    "    plt.plot(-weights[:nums[i]].detach().numpy(), marker=\"o\", markersize=3)\n",
    "    plt.plot(weights[-nums[i]:].detach().numpy()[::-1], marker=\"o\", markersize=3)\n",
    "    plt.xlabel(\"rank\", fontsize=15)\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"abs(weight)\", fontsize=15)\n",
    "        plt.legend([\"positive\", \"negative\"])\n",
    "    plt.title(\"Layer {}\".format(i+1))\n",
    "    \n",
    "    \n",
    "#plt.savefig(\"./fig/mnist_weights.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7324dcaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
