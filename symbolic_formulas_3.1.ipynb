{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6980efa7",
   "metadata": {},
   "source": [
    "# This notebook reproduces the results in Section 3.1 Symbolic Formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02be2108",
   "metadata": {},
   "source": [
    "## Define Brain-inspired Linear layer and MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b02306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from matplotlib.patches import Ellipse, Circle\n",
    "\n",
    "\n",
    "\n",
    "seed = 2\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "#torch.set_default_tensor_type(torch.DoubleTensor)\n",
    "\n",
    "\n",
    "class BioLinear(nn.Module):\n",
    "    # BioLinear is just Linear, but each neuron comes with coordinates.\n",
    "    def __init__(self, in_dim, out_dim, in_fold=1, out_fold=1):\n",
    "        super(BioLinear, self).__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        self.linear = nn.Linear(in_dim, out_dim)\n",
    "        self.in_fold = in_fold # in_fold is the number of folds applied to input vectors. It only affects coordinates, not computations.\n",
    "        self.out_fold = out_fold # out_fold is the number of folds applied to output vectors. It only affects coordinates, not computations.\n",
    "        assert in_dim % in_fold == 0\n",
    "        assert out_dim % out_fold == 0\n",
    "        #compute in_cor, shape: (in_dim)\n",
    "        in_dim_fold = int(in_dim/in_fold)\n",
    "        out_dim_fold = int(out_dim/out_fold)\n",
    "        self.in_coordinates = torch.tensor(list(np.linspace(1/(2*in_dim_fold), 1-1/(2*in_dim_fold), num=in_dim_fold))*in_fold, dtype=torch.float) # place input neurons in 1D Euclidean space\n",
    "        self.out_coordinates = torch.tensor(list(np.linspace(1/(2*out_dim_fold), 1-1/(2*out_dim_fold), num=out_dim_fold))*out_fold, dtype=torch.float) # place output neurons in 1D Euclidean space\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input = x.clone()\n",
    "        self.output = self.linear(x).clone()\n",
    "        return self.output\n",
    "    \n",
    "    \n",
    "class BioMLP(nn.Module):\n",
    "    # BioMLP is just MLP, but each neuron comes with coordinates.\n",
    "    def __init__(self, in_dim=2, out_dim=2, w=2, depth=2, shp=None, token_embedding=False, embedding_size=None):\n",
    "        super(BioMLP, self).__init__()\n",
    "        if shp == None:\n",
    "            shp = [in_dim] + [w]*(depth-1) + [out_dim]\n",
    "            self.in_dim = in_dim\n",
    "            self.out_dim = out_dim\n",
    "            self.depth = depth\n",
    "                 \n",
    "        else:\n",
    "            self.in_dim = shp[0]\n",
    "            self.out_dim = shp[-1]\n",
    "            self.depth = len(shp) - 1\n",
    "\n",
    "        linear_list = []\n",
    "        for i in range(self.depth):\n",
    "            if i == 0:\n",
    "                linear_list.append(BioLinear(shp[i], shp[i+1], in_fold=1))\n",
    "                \n",
    "            else:\n",
    "                linear_list.append(BioLinear(shp[i], shp[i+1]))\n",
    "        self.linears = nn.ModuleList(linear_list)\n",
    "        \n",
    "        \n",
    "        if token_embedding == True:\n",
    "            # embedding size: number of tokens * embedding dimension\n",
    "            self.embedding = torch.nn.Parameter(torch.normal(0,1,size=embedding_size))\n",
    "        \n",
    "        self.shp = shp\n",
    "        # parameters for the bio-inspired trick\n",
    "        self.l0 = 0.1 # distance between two nearby layers\n",
    "        self.in_perm = torch.nn.Parameter(torch.tensor(np.arange(int(self.in_dim/self.linears[0].in_fold)), dtype=torch.float))\n",
    "        self.out_perm = torch.nn.Parameter(torch.tensor(np.arange(int(self.out_dim/self.linears[-1].out_fold)), dtype=torch.float))\n",
    "        self.top_k = 5 # the number of important neurons (used in Swaps)\n",
    "        self.token_embedding = token_embedding\n",
    "        self.n_parameters = sum(p.numel() for p in self.parameters())\n",
    "        self.original_params = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        shp = x.shape\n",
    "        in_fold = self.linears[0].in_fold\n",
    "        x = x.reshape(shp[0], in_fold, int(shp[1]/in_fold))\n",
    "        x = x[:,:,self.in_perm.long()]\n",
    "        x = x.reshape(shp[0], shp[1])\n",
    "        f = torch.nn.SiLU()\n",
    "        for i in range(self.depth-1):\n",
    "            x = f(self.linears[i](x))\n",
    "        x = self.linears[-1](x)\n",
    "        \n",
    "        out_perm_inv = torch.zeros(self.out_dim, dtype=torch.long)\n",
    "        out_perm_inv[self.out_perm.long()] = torch.arange(self.out_dim)\n",
    "        x = x[:,out_perm_inv]\n",
    "        #x = x[:,self.out_perm]\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def get_linear_layers(self):\n",
    "        return self.linears\n",
    "    \n",
    "    def get_cc(self, weight_factor=1.0, bias_penalize=True, no_penalize_last=False):\n",
    "        # compute connection cost\n",
    "        # bias_penalize = True penalizes biases, otherwise doesn't penalize biases\n",
    "        # no_penalize_last = True means do not penalize last linear layer, False means penalize last layer.\n",
    "        cc = 0\n",
    "        num_linear = len(self.linears)\n",
    "        for i in range(num_linear):\n",
    "            if i == num_linear - 1 and no_penalize_last:\n",
    "                weight_factor = 0.\n",
    "            biolinear = self.linears[i]\n",
    "            dist = torch.abs(biolinear.out_coordinates.unsqueeze(dim=1) - biolinear.in_coordinates.unsqueeze(dim=0))\n",
    "            cc += torch.sum(torch.abs(biolinear.linear.weight)*(weight_factor*dist+self.l0))\n",
    "            if bias_penalize == True:\n",
    "                cc += torch.sum(torch.abs(biolinear.linear.bias)*(self.l0))\n",
    "        if self.token_embedding:\n",
    "            cc += torch.sum(torch.abs(self.embedding)*(self.l0))\n",
    "            #pass\n",
    "        return cc\n",
    "    \n",
    "    def swap_weight(self, weights, j, k, swap_type=\"out\"):\n",
    "        # Given a weight matrix, swap the j^th and k^th neuron in inputs/outputs when swap_type = \"in\"/\"out\"\n",
    "        with torch.no_grad():  \n",
    "            if swap_type == \"in\":\n",
    "                temp = weights[:,j].clone()\n",
    "                weights[:,j] = weights[:,k].clone()\n",
    "                weights[:,k] = temp\n",
    "            elif swap_type == \"out\":\n",
    "                temp = weights[j].clone()\n",
    "                weights[j] = weights[k].clone()\n",
    "                weights[k] = temp\n",
    "            else:\n",
    "                raise Exception(\"Swap type {} is not recognized!\".format(swap_type))\n",
    "            \n",
    "    def swap_bias(self, biases, j, k):\n",
    "        # Given a bias vector, swap the j^th and k^th neuron.\n",
    "        with torch.no_grad():  \n",
    "            temp = biases[j].clone()\n",
    "            biases[j] = biases[k].clone()\n",
    "            biases[k] = temp\n",
    "    \n",
    "    def swap(self, i, j, k):\n",
    "        # in the ith layer (of neurons), swap the jth and the kth neuron. \n",
    "        # Note: n layers of weights means n+1 layers of neurons.\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i == 0:\n",
    "            # input layer, only has outgoing weights; update in_perm\n",
    "            weights = linears[i].linear.weight\n",
    "            infold = linears[i].in_fold\n",
    "            fold_dim = int(weights.shape[1]/infold)\n",
    "            for l in range(infold):\n",
    "                self.swap_weight(weights, j+fold_dim*l, k+fold_dim*l, swap_type=\"in\")\n",
    "            # change input_perm\n",
    "            self.swap_bias(self.in_perm, j, k)\n",
    "        elif i == num_linear:\n",
    "            # output layer, only has incoming weights and biases; update out_perm\n",
    "            weights = linears[i-1].linear.weight\n",
    "            biases = linears[i-1].linear.bias\n",
    "            self.swap_weight(weights, j, k, swap_type=\"out\")\n",
    "            self.swap_bias(biases, j, k)\n",
    "            # change output_perm\n",
    "            self.swap_bias(self.out_perm, j, k)\n",
    "        else:\n",
    "            # middle layer : incoming weights, outgoing weights, and biases\n",
    "            weights_in = linears[i-1].linear.weight\n",
    "            weights_out = linears[i].linear.weight\n",
    "            biases = linears[i-1].linear.bias\n",
    "            self.swap_weight(weights_in, j, k, swap_type=\"out\")\n",
    "            self.swap_weight(weights_out, j, k, swap_type=\"in\")\n",
    "            self.swap_bias(biases, j, k)\n",
    "\n",
    "    def get_top_id(self, i, top_k=20):\n",
    "        # in the ith layer (of neurons), get the top k important neurons (have large weight connections with other neurons)\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i == 0:\n",
    "            # input layer\n",
    "            weights = linears[i].linear.weight\n",
    "            score = torch.sum(torch.abs(weights), dim=0)\n",
    "            in_fold = linears[0].in_fold\n",
    "            #print(score.shape)\n",
    "            score = torch.sum(score.reshape(in_fold, int(score.shape[0]/in_fold)), dim=0)\n",
    "        elif i == num_linear:\n",
    "            # output layer\n",
    "            weights = linears[i-1].linear.weight\n",
    "            score = torch.sum(torch.abs(weights), dim=1)\n",
    "        else:\n",
    "            weights_in = linears[i-1].linear.weight\n",
    "            weights_out = linears[i].linear.weight\n",
    "            score = torch.sum(torch.abs(weights_out), dim=0) + torch.sum(torch.abs(weights_in), dim=1)\n",
    "        #print(score.shape)\n",
    "        top_index = torch.flip(torch.argsort(score),[0])[:top_k]\n",
    "        return top_index\n",
    "    \n",
    "    def relocate_ij(self, i, j):\n",
    "        # In the ith layer (of neurons), relocate the jth neuron\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        if i < num_linear:\n",
    "            num_neuron = int(linears[i].linear.weight.shape[1]/linears[i].in_fold)\n",
    "        else:\n",
    "            num_neuron = linears[i-1].linear.weight.shape[0]\n",
    "        ccs = []\n",
    "        for k in range(num_neuron):\n",
    "            self.swap(i,j,k)\n",
    "            ccs.append(self.get_cc())\n",
    "            self.swap(i,j,k)\n",
    "        k = torch.argmin(torch.stack(ccs))\n",
    "        self.swap(i,j,k)\n",
    "            \n",
    "    def relocate_i(self, i):\n",
    "        # Relocate neurons in the ith layer\n",
    "        top_id = self.get_top_id(i, top_k=self.top_k)\n",
    "        for j in top_id:\n",
    "            self.relocate_ij(i,j)\n",
    "            \n",
    "    def relocate(self):\n",
    "        # Relocate neurons in the whole model\n",
    "        linears = self.get_linear_layers()\n",
    "        num_linear = len(linears)\n",
    "        for i in range(num_linear+1):\n",
    "            self.relocate_i(i)\n",
    "            \n",
    "    def plot(self):\n",
    "        fig, ax = plt.subplots(figsize=(3,3))\n",
    "        #ax = plt.gca()\n",
    "        shp = self.shp\n",
    "        s = 1/(2*max(shp))\n",
    "        for j in range(len(shp)):\n",
    "            N = shp[j]\n",
    "            if j == 0:\n",
    "                in_fold = self.linears[j].in_fold\n",
    "                N = int(N/in_fold)\n",
    "            for i in range(N):\n",
    "                if j == 0:\n",
    "                    for fold in range(in_fold):\n",
    "                        circle = Ellipse((1/(2*N)+i/N, 0.1*j+0.02*fold-0.01), s, s/10*((len(shp)-1)+0.4), color='black')\n",
    "                        ax.add_patch(circle)\n",
    "                else:\n",
    "                    for fold in range(in_fold):\n",
    "                        circle = Ellipse((1/(2*N)+i/N, 0.1*j), s, s/10*((len(shp)-1)+0.4), color='black')\n",
    "                        ax.add_patch(circle)\n",
    "\n",
    "\n",
    "        plt.ylim(-0.02,0.1*(len(shp)-1)+0.02)\n",
    "        plt.xlim(-0.02,1.02)\n",
    "\n",
    "        linears = self.linears\n",
    "        for ii in range(len(linears)):\n",
    "            biolinear = linears[ii]\n",
    "            p = biolinear.linear.weight\n",
    "            p_shp = p.shape\n",
    "            p = p/torch.abs(p).max()\n",
    "            in_fold = biolinear.in_fold\n",
    "            fold_num = int(p_shp[1]/in_fold)\n",
    "            for i in range(p_shp[0]):\n",
    "                if ii == 0:\n",
    "                    for fold in range(in_fold):\n",
    "                        for j in range(fold_num):\n",
    "                            plt.plot([1/(2*p_shp[0])+i/p_shp[0], 1/(2*fold_num)+j/fold_num], [0.1*(ii+1),0.1*ii+0.02*fold-0.01], lw=1*np.abs(p[i,j].detach().numpy()), color=\"blue\" if p[i,j]>0 else \"red\")\n",
    "                else:\n",
    "                    for j in range(fold_num):\n",
    "                        plt.plot([1/(2*p_shp[0])+i/p_shp[0], 1/(2*fold_num)+j/fold_num], [0.1*(ii+1),0.1*ii], lw=0.5*np.abs(p[i,j].detach().numpy()), color=\"blue\" if p[i,j]>0 else \"red\")\n",
    "                    \n",
    "        ax.axis('off')\n",
    "        \n",
    "        \n",
    "    def thresholding(self, threshold, checkpoint = True):\n",
    "        # snap too small weights (smaller than threshold) to zero. Useful for pruning.\n",
    "        num = 0\n",
    "        if checkpoint:\n",
    "            self.original_params = [param.clone() for param in self.parameters()]\n",
    "        with torch.no_grad():\n",
    "            for param in self.parameters():\n",
    "                num += torch.sum(torch.abs(param)>threshold)\n",
    "                param.data = param*(torch.abs(param)>threshold)\n",
    "        return num\n",
    "                \n",
    "    def intervening(self, i, pos, value, ptype=\"weight\", checkpoint = True):\n",
    "        if checkpoint:\n",
    "            self.original_params = [param.clone() for param in self.parameters()]\n",
    "        with torch.no_grad():\n",
    "            if ptype == \"weight\":\n",
    "                self.linears[i].linear.weight[pos] = value\n",
    "            elif ptype == \"bias\":\n",
    "                self.linears[i].linear.bias[pos] = value\n",
    "                \n",
    "    def revert(self):\n",
    "        with torch.no_grad():\n",
    "            for param, original_param in zip(self.parameters(), self.original_params):\n",
    "                param.data.copy_(original_param.data)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454e59ac",
   "metadata": {},
   "source": [
    "## The independence example. NN takes in $(x_1,x_2,x_3,x_4)$ and aims to predict $((x_1+x_3)^3, x_2^2+{\\rm sin}(\\pi x_4))$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22477586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# create dataset\n",
    "\n",
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    x1 = data[:,[0]]\n",
    "    x2 = data[:,[1]]\n",
    "    x3 = data[:,[2]]\n",
    "    x4 = data[:,[3]]\n",
    "    out = np.transpose(np.array([(x1+x3)**3, x2**2+np.sin(np.pi*x4)]))\n",
    "    return out\n",
    "\n",
    "\n",
    "d_in = 4\n",
    "d_out = 2\n",
    "\n",
    "inputs = np.random.rand(100,d_in)*2-1\n",
    "labels = f(inputs)\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.float, requires_grad=True)\n",
    "labels = torch.tensor(labels, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "inputs_test = np.random.rand(100,d_in)*2-1\n",
    "labels_test = f(inputs_test)\n",
    "\n",
    "inputs_test = torch.tensor(inputs_test, dtype=torch.float, requires_grad=True)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "\n",
    "width = 20\n",
    "depth = 3\n",
    "shp = [d_in, 20, 20, d_out]\n",
    "\n",
    "\n",
    "model = BioMLP(shp=shp)\n",
    "\n",
    "# train_type = 1; no L1\n",
    "# train_type = 2; L1\n",
    "# train_type = 3: L1 + Local\n",
    "# train_type = 4: L1 + Swap\n",
    "# train_type = 5: L1 + Local + Swap\n",
    "train_type = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, weight_decay=0.0)\n",
    "log = 200\n",
    "lamb = 0 if train_type==1 else 0.001\n",
    "swap_log = 200 if train_type >= 4 else float('inf')\n",
    "weight_factor = 1. if train_type == 3 or train_type == 5 else 0.\n",
    "plot_log = 50\n",
    "steps = 20000\n",
    " \n",
    "for step in range(steps):\n",
    "    \n",
    "    if step == int(steps/4):\n",
    "        lamb *= 10\n",
    "    \n",
    "    if step == int(3*steps/4):\n",
    "        lamb *= 0.1\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pred  = model(inputs)\n",
    "    loss = torch.mean((pred-labels)**2)\n",
    "    pred_test  = model(inputs_test)\n",
    "    loss_test = torch.mean((pred_test-labels_test)**2)\n",
    "    \n",
    "    # do not penalize bias at first (this makes the weight graph look better)\n",
    "    if step < int(3*steps/4):\n",
    "        reg = model.get_cc(bias_penalize=False, weight_factor=weight_factor)\n",
    "    else:\n",
    "        reg = model.get_cc(bias_penalize=True, weight_factor=weight_factor)\n",
    "    #reg = model.get_cc(bias_penalize=True)\n",
    "    total_loss = loss + lamb*reg\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % log == 0:\n",
    "        print(\"step = %d | total loss: %.2e | train loss: %.2e | test loss %.2e | reg: %.2e \"%(step, total_loss.detach().numpy(), loss.detach().numpy(), loss_test.detach().numpy(), reg.detach().numpy()))\n",
    "    \n",
    "    if (step+1) % swap_log == 0:\n",
    "        model.relocate()\n",
    "\n",
    "    if step % plot_log == 0:\n",
    "        model.plot()\n",
    "        formulas = [r\"   $(x_1+x_3)^3$\", r\"$x_2^2+{\\rm sin}(\\pi x_4)$\"]\n",
    "        fontsize = 12\n",
    "        for j in range(shp[0]):\n",
    "            plt.text(1/(2*shp[1])+5*j/shp[1]+0.04, -0.04, \"$x_{}$\".format(model.in_perm[j].long()+1), fontsize=fontsize)\n",
    "\n",
    "        for j in range(shp[-1]):\n",
    "            plt.text(1/(2*shp[0])+2*j/shp[0]-0.1, 0.1*(len(shp)-1)+0.02, formulas[model.out_perm[j].long()], fontsize=fontsize)\n",
    "\n",
    "        #plt.title(\"(a) independence\", y=1.1,fontsize=fontsize)\n",
    "        #plt.savefig(\"./video_figs/sf_id/{0:05d}.png\".format(step))\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b33fcd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.plot([-6,6],[-6,6],ls=\"--\",color=\"red\", alpha=0.3)\n",
    "plt.scatter(labels_test.detach().numpy(), pred_test.detach().numpy(), s=5)\n",
    "plt.xlim(-5,5)\n",
    "plt.ylim(-5,5)\n",
    "plt.xlabel(\"true\")\n",
    "plt.ylabel(\"predicted\")\n",
    "plt.title(\"independence\\n test MSE = 7.4e-3\")\n",
    "plt.savefig(\"./fig/independence_compare.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f203a",
   "metadata": {},
   "source": [
    "## The feature sharing example. NN takes in $(x_1,x_2,x_3)$ and aims to predict $(x_1^2,x_1^2+x_2^2,x_1^2+x_2^2+x_3^2)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400cf739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# create dataset\n",
    "\n",
    "seed = 4\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    x1 = data[:,[0]]\n",
    "    x2 = data[:,[1]]\n",
    "    x3 = data[:,[2]]    \n",
    "    out = np.transpose(np.array([x1**2, x1**2+x2**2, x1**2+x2**2+x3**2]))\n",
    "    return out\n",
    "\n",
    "\n",
    "d_in = 3\n",
    "d_out = 3\n",
    "\n",
    "inputs = np.random.rand(100,d_in)*2-1\n",
    "labels = f(inputs)\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.float, requires_grad=True)\n",
    "labels = torch.tensor(labels, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "inputs_test = np.random.rand(100,d_in)*2-1\n",
    "labels_test = f(inputs_test)\n",
    "\n",
    "inputs_test = torch.tensor(inputs_test, dtype=torch.float, requires_grad=True)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "\n",
    "width = 20\n",
    "depth = 3\n",
    "shp = [d_in, 20, 20, d_out]\n",
    "\n",
    "\n",
    "model = BioMLP(shp=shp)\n",
    "\n",
    "# train_type = 1; no L1\n",
    "# train_type = 2; L1\n",
    "# train_type = 3: L1 + Local\n",
    "# train_type = 4: L1 + Swap\n",
    "# train_type = 5: L1 + Local + Swap\n",
    "train_type = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, weight_decay=0.0)\n",
    "log = 200\n",
    "lamb = 0 if train_type==1 else 0.001\n",
    "swap_log = 200 if train_type >= 4 else float('inf')\n",
    "weight_factor = 1. if train_type == 3 or train_type == 5 else 0.\n",
    "plot_log = 50\n",
    "steps = 20000\n",
    " \n",
    "for step in range(steps):\n",
    "    \n",
    "    if step == int(steps/4):\n",
    "        lamb *= 10\n",
    "    \n",
    "    if step == int(3*steps/4):\n",
    "        lamb *= 0.1\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pred  = model(inputs)\n",
    "    loss = torch.mean((pred-labels)**2)\n",
    "    pred_test  = model(inputs_test)\n",
    "    loss_test = torch.mean((pred_test-labels_test)**2)\n",
    "    \n",
    "    # do not penalize bias at first (this makes the weight graph look better)\n",
    "    if step < int(3*steps/4):\n",
    "        reg = model.get_cc(bias_penalize=False, weight_factor=weight_factor)\n",
    "    else:\n",
    "        reg = model.get_cc(bias_penalize=True, weight_factor=weight_factor)\n",
    "    #reg = model.get_cc(bias_penalize=True)\n",
    "    total_loss = loss + lamb*reg\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % log == 0:\n",
    "        print(\"step = %d | total loss: %.2e | train loss: %.2e | test loss %.2e | reg: %.2e \"%(step, total_loss.detach().numpy(), loss.detach().numpy(), loss_test.detach().numpy(), reg.detach().numpy()))\n",
    "    \n",
    "    if (step+1) % swap_log == 0:\n",
    "        model.relocate()\n",
    "\n",
    "    if step % plot_log == 0:\n",
    "        model.plot()\n",
    "        formulas = [r\"$x_1^2$\", r\"$x_1^2+x_2^2$\", r\"$x_1^2+x_2^2+x_3^2$\"]\n",
    "        fontsize = 12\n",
    "        for j in range(shp[0]):\n",
    "            plt.text(1/(2*shp[1])+7*j/shp[1]+0.1, -0.04, \"$x_{}$\".format(model.in_perm[j].long()+1), fontsize=fontsize)\n",
    "\n",
    "        for j in range(shp[-1]):\n",
    "            plt.text(1/(2*shp[0])+j/shp[0]-0.12, 0.1*(len(shp)-1)+0.02, formulas[model.out_perm[j].long()], fontsize=fontsize)\n",
    "\n",
    "        #plt.savefig(\"./video_figs/sf_fs/{0:05d}.png\".format(step))\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f279d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.plot([-6,6],[-6,6],ls=\"--\",color=\"red\", alpha=0.3)\n",
    "plt.scatter(labels_test.detach().numpy(), pred_test.detach().numpy(), s=5)\n",
    "plt.xlim(-1,3)\n",
    "plt.ylim(-1,3)\n",
    "plt.xlabel(\"true\")\n",
    "plt.ylabel(\"predicted\")\n",
    "plt.title(\"feature sharing\\n test MSE = 8.5e-5\")\n",
    "plt.savefig(\"./fig/feature_sharing_compare.png\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dda868f",
   "metadata": {},
   "source": [
    "## The independence example. NN takes in $(x_1,x_2,x_3,x_4)$ and aims to predict $\\sqrt{(x_1-x_2)^2+(x_3-x_4)}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43daad44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# create dataset\n",
    "\n",
    "seed = 3\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    x1 = data[:,[0]]\n",
    "    x2 = data[:,[1]]\n",
    "    x3 = data[:,[2]]\n",
    "    x4 = data[:,[3]]\n",
    "    #out = np.transpose(np.array([(x1+x3)**3, x2**2+np.sin(np.pi*x4)]))\n",
    "    #out = np.transpose(np.array([x1**2, x1**2+x2**2, x1**2+x2**2+x3**2]))\n",
    "    out = np.transpose(np.array([np.sqrt((x1-x2)**2+(x3-x4)**2)]))\n",
    "    return out\n",
    "\n",
    "\n",
    "d_in = 4\n",
    "d_out = 1\n",
    "\n",
    "inputs = np.random.rand(100,d_in)*2-1\n",
    "labels = f(inputs)\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.float, requires_grad=True)\n",
    "labels = torch.tensor(labels, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "inputs_test = np.random.rand(100,d_in)*2-1\n",
    "labels_test = f(inputs_test)\n",
    "\n",
    "inputs_test = torch.tensor(inputs_test, dtype=torch.float, requires_grad=True)\n",
    "labels_test = torch.tensor(labels_test, dtype=torch.float, requires_grad=True)\n",
    "\n",
    "\n",
    "width = 20\n",
    "depth = 5\n",
    "shp = [d_in, 20, 20, 20, 20, d_out]\n",
    "\n",
    "\n",
    "model = BioMLP(shp=shp)\n",
    "\n",
    "# train_type = 1; no L1\n",
    "# train_type = 2; L1\n",
    "# train_type = 3: L1 + Local\n",
    "# train_type = 4: L1 + Swap\n",
    "# train_type = 5: L1 + Local + Swap\n",
    "train_type = 5\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.002, weight_decay=0.0)\n",
    "log = 200\n",
    "lamb = 0 if train_type==1 else 0.001\n",
    "swap_log = 200 if train_type >= 4 else float('inf')\n",
    "weight_factor = 1. if train_type == 3 or train_type == 5 else 0.\n",
    "plot_log = 50\n",
    "steps = 20000\n",
    " \n",
    "for step in range(steps):\n",
    "    \n",
    "    if step == int(steps/4):\n",
    "        lamb *= 3\n",
    "    \n",
    "    if step == int(3*steps/4):\n",
    "        lamb *= 1\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    pred  = model(inputs)\n",
    "    loss = torch.mean((pred-labels)**2)\n",
    "    pred_test  = model(inputs_test)\n",
    "    loss_test = torch.mean((pred_test-labels_test)**2)\n",
    "    \n",
    "    # do not penalize bias at first (this makes the weight graph look better)\n",
    "    if step < int(3*steps/4):\n",
    "        reg = model.get_cc(bias_penalize=False, weight_factor=weight_factor)\n",
    "    else:\n",
    "        reg = model.get_cc(bias_penalize=True, weight_factor=weight_factor)\n",
    "    #reg = model.get_cc(bias_penalize=True)\n",
    "    total_loss = loss + lamb*reg\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if step % log == 0:\n",
    "        print(\"step = %d | total loss: %.2e | train loss: %.2e | test loss %.2e | reg: %.2e \"%(step, total_loss.detach().numpy(), loss.detach().numpy(), loss_test.detach().numpy(), reg.detach().numpy()))\n",
    "    \n",
    "    if (step+1) % swap_log == 0:\n",
    "        model.relocate()\n",
    "\n",
    "    if step % plot_log == 0:\n",
    "        model.plot()\n",
    "        formulas = [r\"$\\sqrt{(x_1-x_2)^2+(x_3-x_4)^2}$\"]\n",
    "        fontsize = 12\n",
    "        for j in range(shp[0]):\n",
    "            plt.text(1/(2*shp[1])+5*j/shp[1]+0.04, -0.05, \"$x_{}$\".format(model.in_perm[j].long()+1), fontsize=fontsize)\n",
    "\n",
    "        for j in range(shp[-1]):\n",
    "            plt.text(1/(2*shp[0])+j/shp[0]-0.07, 0.1*(len(shp)-1)+0.02, formulas[model.out_perm[j].long()], fontsize=fontsize)\n",
    "    \n",
    "        #plt.savefig(\"./video_figs/sf_comp/{0:05d}.png\".format(step))\n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e60bcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,3))\n",
    "plt.plot([-6,6],[-6,6],ls=\"--\",color=\"red\", alpha=0.3)\n",
    "plt.scatter(labels_test.detach().numpy(), pred_test.detach().numpy(), s=5)\n",
    "plt.xlim(-1,3)\n",
    "plt.ylim(-1,3)\n",
    "plt.xlabel(\"true\")\n",
    "plt.ylabel(\"predicted\")\n",
    "plt.title(\"compositionality\\n test MSE = 1.3e-3\")\n",
    "plt.savefig(\"./fig/compositionality_compare.png\", bbox_inches=\"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
